print("ðŸ”¥ API STARTED FROM:", __file__)

"""
PromptGuard API â€” Cloud Deployment Ready
----------------------------------------
âœ“ Works on Railway / Render / Fly.io
âœ“ Dynamic PORT support
âœ“ Safe backend imports
âœ“ Gemini optional
"""

import os
import sys
import requests
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv

# -------------------------------------------------------
# Ensure backend folder is importable (Railway compatible)
# -------------------------------------------------------
BASE_DIR = os.path.dirname(os.path.abspath(__file__))     # /backend
ROOT_DIR = os.path.dirname(BASE_DIR)                      # project root
if ROOT_DIR not in sys.path:
    sys.path.append(ROOT_DIR)

# -------------------------------------------------------
# Import analyzer AFTER sys.path fix
# -------------------------------------------------------
from backend.detectors.analyzer import analyze_prompt

# -------------------------------------------------------
# Load env variables (Gemini optional)
# -------------------------------------------------------
load_dotenv()
API_KEY = os.getenv("GEMINI_API_KEY")

if not API_KEY:
    print("âš  No GEMINI_API_KEY found â€” running in LOCAL MODE ONLY.\n")

# -------------------------------------------------------
# FastAPI App
# -------------------------------------------------------
app = FastAPI(
    title="PromptGuard API",
    version="2.0.1",
    description="AI Prompt Firewall combining rules + semantic analysis.",
)

# CORS for frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],   # allow all (frontend will run on separate domain)
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# -------------------------------------------------------
# Request Model
# -------------------------------------------------------
class PromptRequest(BaseModel):
    prompt: str

# -------------------------------------------------------
# Routes
# -------------------------------------------------------
@app.get("/")
def health():
    return {"status": "OK", "message": "PromptGuard API is alive ðŸ”¥"}

@app.post("/analyze")
def analyze_route(data: PromptRequest):

    prompt = data.prompt
    analysis = analyze_prompt(prompt)

    # Block unsafe prompts
    if not analysis["final_safe"]:
        return {
            "safe": False,
            "analysis": analysis,
            "response": "ðŸš« Unsafe prompt blocked by PromptGuard.",
        }

    # If safe + Gemini not configured â†’ local only
    if not API_KEY:
        return {
            "safe": True,
            "analysis": analysis,
            "response": "âš  Gemini not configured â€” only local AI analysis executed.",
        }

    # If Gemini exists â†’ call Gemini safely
    try:
        url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent"
        payload = {
            "contents": [{"parts": [{"text": prompt}]}]
        }

        headers = {"Content-Type": "application/json"}

        res = requests.post(f"{url}?key={API_KEY}", json=payload, headers=headers)

        if res.status_code != 200:
            raise HTTPException(status_code=res.status_code, detail=res.text)

        g = res.json()
        text = (
            g.get("candidates", [{}])[0]
             .get("content", {})
             .get("parts", [{}])[0]
             .get("text", "âš  Gemini returned no text.")
        )

        return {"safe": True, "analysis": analysis, "response": text}

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Gemini API error: {e}")

# -------------------------------------------------------
# Local development runner (Railway ignores this)
# -------------------------------------------------------
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=int(os.getenv("PORT", 9000))   # railway sets PORT automatically
    )
